{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a CNN on CIFAR-10 using TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "**Note:** You can find the code for this post [here](https://github.com/vbvsharma/build-cnn-on-cifar10-using-tensorflow).\n",
    "\n",
    "[The CIFAR-10 dataset](http://www.cs.toronto.edu/~kriz/cifar.html) is a standard dataset used in computer vision and deep learning community. It consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. The mapping of all 0-9 integers to class labels is listed below.:\n",
    "\n",
    "- 0 ~> Airplane\n",
    "- 1 ~> Automobile\n",
    "- 2 ~> Bird\n",
    "- 3 ~> Cat\n",
    "- 4 ~> Deer\n",
    "- 5 ~> Dog\n",
    "- 6 ~> Frog\n",
    "- 7 ~> Horse\n",
    "- 8 ~> Ship\n",
    "- 9 ~> Truck\n",
    "\n",
    "It is a fairly simple dataset. Hence, it provides the flexibility to play with various techniques, suh as hyperparameter tuning, regularization, training-test split, parameter search, etc. Therefore, I encourage the reader to play with this dataset after reading this tutorial.\n",
    "\n",
    "In this tutorial, we will build a convolutional neural network model from scratch using TensorFlow, train that model and then evaluate its performance on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore CIFAR-10 dataset\n",
    "\n",
    "Let us load the dataset. The dataset is split into training and testing sets. The training set consists of 50000 images, with 5000 images of each class, and the testing set consists of 10000 images, with 1000 images from each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import the CIFAR-10 dataset from keras' datasets\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "# Import this PyPlot to visualize images\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Load dataset\n",
    "(X_train, Y_train), (X_test, Y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shapes of training and testing set\n",
    "print(\"X_train.shape =\", X_train.shape, \"Y_train.shape =\", Y_train.shape)\n",
    "print(\"X_test.shape =\", X_test.shape, \"Y_test.shape =\", Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can tell from the shapes that,\n",
    "\n",
    "- **X_train** has 50000 training images, each 32 pixel wide, 32 pixel high, and 3 color channels\n",
    "- **X_test** has 10000 testing images, each 32 pixel wide, 32 pixel high, and 3 color channels\n",
    "- **Y_train** has 50000 labels\n",
    "- **Y_test** has 10000 labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define constants for number of classes and its labels, to make the code more readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 10\n",
    "CIFAR10_CLASSES = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \n",
    "                   \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets look at some random images from the training set. You can change the number of columns and rows to get more/less images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show random images from training set\n",
    "cols = 8 # Number of columns\n",
    "rows = 4 # Number of rows\n",
    "\n",
    "fig = plt.figure(figsize=(2 * cols, 2 * rows))\n",
    "\n",
    "# Add subplot for each random image\n",
    "for col in range(cols):\n",
    "    for row in range(rows):\n",
    "        random_index = np.random.randint(0, len(Y_train)) # Pick a random index for sampling the image\n",
    "        ax = fig.add_subplot(rows, cols, col * rows + row + 1) # Add a sub-plot at (row, col)\n",
    "        ax.grid(b=False) # Get rid of the grids\n",
    "        ax.axis(\"off\") # Get rid of the axis\n",
    "        ax.imshow(X_train[random_index, :]) # Show random image\n",
    "        ax.set_title(CIFAR10_CLASSES[Y_train[random_index][0]]) # Set title of the sub-plot\n",
    "plt.show() # Show the image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Training and Testing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before defining the model and training the model, let us prepare the training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "print(\"TensorFlow's version is\", tf.__version__)\n",
    "print(\"Keras' version is\", tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the inputs, to train the model faster and prevent exploding gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize training and testing pixel values\n",
    "X_train_normalized = X_train / 255 - 0.5\n",
    "X_test_normalized = X_test / 255 - 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the labels to one-hot coded vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert class vectors to binary class matrices.\n",
    "Y_train_coded = tf.keras.utils.to_categorical(Y_train, NUM_CLASSES)\n",
    "Y_test_coded = tf.keras.utils.to_categorical(Y_test, NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Convolutional Neural Network Model\n",
    "\n",
    "Next, let us define a model that takes images as input, and outputs class probabilities.\n",
    "\n",
    "You can learn more about the implementation details\n",
    "https://keras.io.\n",
    "\n",
    "We will define following layers in the model:\n",
    "\n",
    "- **Convolutional layer** which takes (32, 32, 3) shaped images as input, outputs 16 filters, and has a kernel size of (3, 3), with the same padding, and uses LeakyReLU as activation function\n",
    "- **Convolutional layer** which takes (32, 32, 16) shaped tensor as input, outputs 32 filters, and has a kernel size of (3, 3), with the same padding, and uses LeakyReLU as activation function\n",
    "- **Max Pool layer** with pool size of (2, 2), this outputs (16, 16, 16) tensor\n",
    "- **Dropout layer** with the dropout rate of 0.25, to prevent overfitting\n",
    "- **Convolutional layer** which takes (16, 16, 16) shaped tensor as input, outputs 32 filters, and has a kernel size of (3, 3), with the same padding, and uses LeakyReLU as activation function\n",
    "- **Convolutional layer** which takes (16, 16, 32) shaped tensor as input, outputs 64 filters, and has a kernel size of (3, 3), with the same padding, and uses LeakyReLU as activation function\n",
    "- **Max Pool layer** with pool size of (2, 2), this outputs (8, 8, 64) tensor\n",
    "- **Dropout layer** with the dropout rate of 0.25, to prevent overfitting\n",
    "- **Dense layer** which takes input from 8x8x64 neurons, and has 256 neurons\n",
    "- **Dropout layer** with the dropout rate of 0.5, to prevent overfitting\n",
    "- **Dense layer** with 10 neurons, and softmax activation, is the final layer\n",
    "\n",
    "As you can see, all the layers use LeakyReLU activations, except the last layer. This is a pretty good choice most of the time, but you change these as well to play with other activations such as tanh, sigmoid, ReLU, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary building blocks\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Activation, Dropout\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "\n",
    "def make_model():\n",
    "    \"\"\"\n",
    "    Define your model architecture here.\n",
    "    Returns `Sequential` model.\n",
    "    \"\"\"\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(filters=16, kernel_size=(3, 3), padding='same', input_shape=(32, 32, 3)))\n",
    "    model.add(LeakyReLU(0.1))\n",
    "    \n",
    "    model.add(Conv2D(filters=32, kernel_size=(3, 3), padding='same'))\n",
    "    model.add(LeakyReLU(0.1))\n",
    "    \n",
    "    model.add(MaxPooling2D())\n",
    "    \n",
    "    model.add(Dropout(rate=0.25))\n",
    "    \n",
    "    model.add(Conv2D(filters=32, kernel_size=(3, 3), padding='same'))\n",
    "    model.add(LeakyReLU(0.1))\n",
    "    \n",
    "    model.add(Conv2D(filters=64, kernel_size=(3, 3), padding='same'))\n",
    "    model.add(LeakyReLU(0.1))\n",
    "    \n",
    "    model.add(MaxPooling2D())\n",
    "    \n",
    "    model.add(Dropout(rate=0.25))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(units=256))\n",
    "    model.add(LeakyReLU(0.1))\n",
    "    \n",
    "    model.add(Dropout(rate=0.5))\n",
    "    \n",
    "    model.add(Dense(units=10))\n",
    "    model.add(Activation(\"softmax\"))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe model\n",
    "s = tf.keras.backend.clear_session()\n",
    "model = make_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train your model\n",
    "\n",
    "Next, we train the model that we defined above. We will use 0.005 as our initial learning rate, training batch size will be 64, we will train our model for 10 epochs. Feel free to change these hyperparameters, to dive deeper and know their effects. We use categorical cross entropy loss as our lass function and Adamax optimizer for convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INIT_LR = 5e-3  # initial learning rate\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "\n",
    "s = tf.keras.backend.clear_session()  # clear default graph\n",
    "# don't call K.set_learning_phase() !!! (otherwise will enable dropout in train/test simultaneously)\n",
    "model = make_model()  # define our model\n",
    "\n",
    "# prepare model for fitting (loss, optimizer, etc)\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',  # we train 10-way classification\n",
    "    optimizer=tf.keras.optimizers.Adamax(lr=INIT_LR),  # for SGD\n",
    "    metrics=['accuracy']  # report accuracy during training\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a learning rate scheduler, which decays learning rate after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# scheduler of learning rate (decay with epochs)\n",
    "def lr_scheduler(epoch):\n",
    "    return INIT_LR * 0.9 ** epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define a class that handles callbacks from keras. It prints out the learning rate used in that epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callback for printing of actual learning rate used by optimizer\n",
    "class LrHistory(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        print(\"Learning rate:\", tf.keras.backend.get_value(model.optimizer.lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us train our model on normalized X_train, **X_train_normalized**, and one-hot coded matrix, **Y_train_coded**. During training we will also keep validating on, **X_test_normalized** and **Y_train_coded**. In this way we can keep an eye on model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "history = model.fit(\n",
    "    X_train_normalized, Y_train_coded,  # prepared data\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[tf.keras.callbacks.LearningRateScheduler(lr_scheduler), \n",
    "               LrHistory()],\n",
    "    validation_data=(X_test_normalized, Y_test_coded),\n",
    "    shuffle=True,\n",
    "    verbose=1,\n",
    "    initial_epoch=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model):# serialize model to JSON\n",
    "    model_json = model.to_json()\n",
    "    with open(\"model.json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(\"model.h5\")\n",
    "    print(\"Saved model to disk\")\n",
    "\n",
    "\n",
    "save_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model\n",
    "\n",
    "Now that we have trained our model, let us see how it performs.\n",
    "\n",
    "Let us load the saved model from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    from tensorflow.keras.models import model_from_json\n",
    "    \n",
    "    # load json and create model\n",
    "    json_file = open('model.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    loaded_model.load_weights(\"model.h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "    \n",
    "    return loaded_model\n",
    "\n",
    "model = load_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at the learning curve during the training of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us predict the classes for each image in testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make test predictions\n",
    "Y_pred_test = model.predict_proba(X_test_normalized) # Predict probability of image belonging to a class, for each class\n",
    "Y_pred_test_classes = np.argmax(Y_pred_test, axis=1) # Class with highest probability from predicted probabilities\n",
    "Y_test_classes = np.argmax(Y_test_coded, axis=1) # Actual class\n",
    "Y_pred_test_max_probas = np.max(Y_pred_test, axis=1) # Highest probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at the confusion matrix to understand the performance of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix and accuracy\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "plt.figure(figsize=(7, 6))\n",
    "plt.title('Confusion matrix', fontsize=16)\n",
    "plt.imshow(confusion_matrix(Y_test_classes, Y_pred_test_classes))\n",
    "plt.xticks(np.arange(10), CIFAR10_CLASSES, rotation=45, fontsize=12)\n",
    "plt.yticks(np.arange(10), CIFAR10_CLASSES, fontsize=12)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "print(\"Test accuracy:\", accuracy_score(Y_test_classes, Y_pred_test_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at some random predictions from our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect preditions\n",
    "cols = 8\n",
    "rows = 2\n",
    "fig = plt.figure(figsize=(2 * cols - 1, 3 * rows - 1))\n",
    "for i in range(cols):\n",
    "    for j in range(rows):\n",
    "        random_index = np.random.randint(0, len(Y_test))\n",
    "        ax = fig.add_subplot(rows, cols, i * rows + j + 1)\n",
    "        ax.grid(b=False)\n",
    "        ax.axis('off')\n",
    "        ax.imshow(X_test[random_index, :])\n",
    "        pred_label = CIFAR10_CLASSES[Y_pred_test_classes[random_index]]\n",
    "        pred_proba = Y_pred_test_max_probas[random_index]\n",
    "        true_label = CIFAR10_CLASSES[Y_test[random_index][0]]\n",
    "        ax.set_title(\"pred: {}\\nscore: {:.3}\\ntrue: {}\".format(\n",
    "               pred_label, pred_proba, true_label\n",
    "        ))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, we discovered how to develop a convolutional neural network for CIFAR-10 classification from scratch using TensorFlow.\n",
    "\n",
    "Specifically, we learned:\n",
    "\n",
    "- How to load CIFAR-10 in your python program\n",
    "- How to look at random images in the dataset\n",
    "- How to define and train a model\n",
    "- How to save the learnt weights of the model to disk\n",
    "- How to predict clsses using the model\n",
    "\n",
    "\n",
    "These topics will be covered later:\n",
    "\n",
    "- How to improve your model\n",
    "- How to thoroughly validate your model\n",
    "\n",
    "This is a pretty good model (if it is among your first few), but people have achieved around 99% accuracy in this dataset. You can checkout other people's performance on this dataset [here](https://benchmarks.ai/cifar-10)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to work on this model on your system, you can find th code [here](https://github.com/vbvsharma/build-cnn-on-cifar10-using-tensorflow)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
